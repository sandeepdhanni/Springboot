apache kafka is a open-source distributed streaming platform it allows u to publish or subscribe to streams of records,similar 
to the messaging system..one hand u can send and other hand u can consume that message..
processing real time stream of data...
sending a stream of contineous data from the application to kafka server is called creating or generating real time stream of data..
(distributed streaming platform)-means creating real time stream and processing real time stream.


apache kafka was originally developed at linkedin and was open sourced in 2011..



DIFFERENCE BETWEEN KAFKA AND RABBITMQ -
RABBITMQ -
âœ… Low Latency â€“ Messages are pushed instantly to consumers.
âœ… Guaranteed Delivery â€“ Messages are processed only once with acknowledgments.
âœ… Complex Routing â€“ Direct messages to different consumers (restaurants, delivery agents, customers).
âœ… Request-Response Handling â€“ Microservices communicate effectively with RabbitMQ.

kafka -
âœ… High Throughput â€“ Kafka handles millions of events per second.
âœ… Event Streaming â€“ Kafka allows real-time event-driven architectures.
âœ… Message Retention â€“ Unlike RabbitMQ, Kafka retains messages even after they are read.
âœ… Scalability â€“ Kafka efficiently scales horizontally across multiple brokers.




Feature	                          RabbitMQ	                                                             Kafka
Message Model	            Queue-based (push)	                                            Pub-Sub with partitions (pull)
Latency	                    Low latency (milliseconds)	                                  Higher latency (millisecond-second range)
Throughput	            Lower (thousands of messages/sec)	                          Higher (millions of messages/sec)
Retention	          Messages are removed after consumption	                 Messages persist for a configurable period
Ordering	            Guaranteed in a single queue	                           Guaranteed within a partition
Scalability	            Harder to scale  	                                             Horizontally scalable
Processing Model	    Ideal for tasks and work queues	                         Ideal for event-driven architectures
Use Case	      Real-time messaging (e.g., notifications, transactions)         	Log processing, analytics, event sourcing



why kafka is needed?


because we need a distributed platform that can process all the data in parallel and you can also send data easily
 and retrieve data on the other end..



 apache kafka has three main component ..they are producer,consumer and broker
producers are the source of the data pushing the records into the data..
consumers are the people that consume the data to do something about it,basically to take action




kafka components:
producer
consumer
broker
cluster
topic
patitions
offset
consumer groups
zookeeper

1. Producer (Who sends messages?)
A producer is an application that publishes (writes) messages to a Kafka topic.
Producers do not send messages to consumers directly. Instead, they send messages to a Kafka broker.
Kafka allows multiple producers to write to the same topic.
Example
ðŸš– In a ride-sharing app (like Uber), each driver updates their location every second.
The driverâ€™s mobile app is a producer.
It sends a location update event to the "driver-locations" topic in Kafka.

2. Consumer (Who reads messages?)
A consumer is an application that subscribes to a Kafka topic and reads messages.
Consumers do not pull messages randomly. Instead, they poll Kafka brokers for new messages.
Consumers commit offsets (keep track of which messages they have read).
Example
ðŸš– The ride-matching service reads from the "driver-locations" topic to match nearby riders.

3. Broker (The Kafka server)
A Kafka broker is a server that stores and manages messages in topics.it is a virtual machine that runs the kafka service..
Each broker handles a portion of Kafka's total workload.
Kafka clusters have multiple brokers working together for scalability and fault tolerance.
Example
If Uber receives 100,000 ride requests per second, one broker cannot handle this load alone.
Instead, the data is distributed across multiple brokers.

4. Cluster (Group of brokers)
A Kafka cluster is a group of multiple Kafka brokers working together.kafka runs as a cluster..
Clustering ensures high availability and fault tolerance.
If one broker fails, another broker takes over.
Example
Netflix uses hundreds of Kafka brokers in a cluster to handle billions of real-time streaming events daily.

5. Topic (The category of messages)
A topic is like a logical channel where producers send messages and consumers read from.
Each topic can have multiple partitions.
Kafka topics support multiple consumers reading in parallel.
Example
ðŸš– Kafka topic in Uber:
"ride-requests" â†’ Stores ride requests from users.
"driver-locations" â†’ Stores real-time driver location updates.
"payments" â†’ Stores ride payment transaction details.

6. Partitions (For scalability and parallelism)
Kafka divides topics into partitions to scale horizontally.
Each partition stores a subset of the topicâ€™s messages.
Kafka guarantees message order within a partition, but not across partitions.
Example
ðŸš– The "ride-requests" topic is split into 3 partitions:
Partition 0 â†’ Requests from New York.
Partition 1 â†’ Requests from Los Angeles.
Partition 2 â†’ Requests from Chicago.
If 3 consumers are reading this topic, each will process one partition in parallel, improving speed.


7. Offset (Keeps track of messages)
Each message in a Kafka partition has a unique offset (like an ID).
Consumers use offsets to track which messages they have already processed.
If a consumer restarts, it can resume from the last saved offset.
Example
ðŸš– A ride-matching service crashes and restarts.
It resumes reading from offset 2500 instead of starting over.

8. Consumer Groups (Multiple consumers working together)
A consumer group is a set of consumers working together to process a topic in parallel.
Each consumer reads from different partitions, so they donâ€™t duplicate work.
Kafka ensures one partition is read by only one consumer in the group.
Example
ðŸš– The ride-matching service has 3 instances, so it forms a consumer group.
Consumer A â†’ Reads Partition 0 (New York requests).
Consumer B â†’ Reads Partition 1 (Los Angeles requests).
Consumer C â†’ Reads Partition 2 (Chicago requests).
If Consumer B fails, Kafka reassigns Partition 1 to another consumer automatically.

9. Zookeeper (Kafkaâ€™s manager)
Zookeeper is responsible for managing Kafka brokers and maintaining cluster metadata.
It tracks which brokers are alive and handles leader election for partitions.
Zookeeper coordinates broker failures and ensures reliability.
Example
ðŸš– If a Kafka broker crashes, Zookeeper automatically reassigns partitions to a healthy broker.



producer:producer is the source of data who will publish the messages or events..
consumer:consumer act as a receiver.it is responsible for receiving or consuming a message
broker: but producer and consumer cannot directly commmunicate with each other..there should
   be a middlemen to process the messages from producer to consumer that is kafka server..
   this is message broker..
zookeeper:it is a prerelinques for kafka..kafka is a distributive system and it uses zookeper
   for coordination and to track the the status of the kafka cluster nodes.it also keeps track
   of kafka topics,partitions,offset etc.



kafka installation:
there are three ways for instalation:
opensource:apache kafka
commercial distribution:confluent kafka
managed kafka service:confluent and aws

opensource:apache kafka- this u can easiy download from apache portal and u can install and use it.
commercial distribution:confluent kafka- this comes with a lot of tools and utility to perform a day
                       to day operation..this will cost an organisation or for project..confluent 
                        kafka is the best commercial destribution to use which simplifies connecting
                       datasource to the kafka,building streaming application as well as securing,
                       managing your kafka infra.
managed kafka service:confluent and aws- this comes with everything u need.u just need to create an
                      instance as per requirement u need and let everything managed by the kafka 
                      provider.


1st step -opensource instalation
https://kafka.apache.org/downloads
dowload the latest version near binary downloads.click on any scala


2nd process - commercial distribution:confluent kafka
https://www.confluent.io/confluent-cloud/tryfree/
fill in the details and under the community version click on download 


after downloading any one process then download kafka offset explorer for monitring kafka messaging 
system..
https://www.kafkatool.com/download.html
click on download
and double click after downloaded..


installation process for open  source kafka 
next step:
first we need to start the kafka ecosystem fot that we need a kafka server or broker.or to manage the
server we need a server which is zookeeper.steps are :
1.start zookeeper
2.start kafka server/broker
3.create a topic  

if producer want to send any message to consumer then he cannot so we need a  component called topic
so that producer can send it to topic and consumer can consume it.once u have created a topic u can 
define n number of partitions to distribute the load comming from producer.

next using the command line we will start the zookeeper,kafka server,create the topic..
go the folder where u have downloaded the kafka 
go to kafka folder and open the terminal from that folder..create 3 terminals..
first start the zookeeper...there is a shellscript file in bin directory-zookeeper-server-start.sh
run the command:
bin/zookeeper-server-start.sh config/zookeeper.properties

zookeeper port number is 2181

next:
in the next terminal start the kafka server.
run command:
bin/kafka-server-start.sh config/server.properties
kafka server/broker port number is 9092.


next is to create a topic:
open another terminal and run command:
the bootstrap server is nothing but the kafka server which u have run and then the port number and the 
topic name and partition how many u want or by default this will take 1 and replication means copy...

bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partition 3 --replication-factor 1

if u want the list of topics run this command:
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list


if u want to describe about the topic then run this command:
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test


next step is to open the offset explorer for testing
after opening u will find the topics which u have created click on any because need to create a 
connection ..
create a new connection by clicking right click and new connection and then test it..


next step to run the producer and consumer
run command 
bin/kafka-console-producer.sh localhost:9092 --topic test


and then run the consumer and run command:
bin/kafka-console-consumer.sh --bootstrap -server localhost:9092 --topic --test --from-beginning

after that enter some message in producer it will display in the consumer


u can also test it like 
f u have any huge amount of data just give the path
bin/kafka-console-producer.sh localhost:9092 --topic test </users/onedrive/Downloads/test.csv








installation process for commercial distribution:confluent kafka community adition
to start the confluent kafka u need to stop the open source kafka

the steps are same as like open source kafka
first start zookeeper
bin/zookeeper-server-start.sh etc/kafka/zookeeper.properties


start kafka server:
bin/kafka-server-start.sh etc/kafka/server.properties


create topic:
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partition 3 --replication-factor 1

this is the github link where u can find the commands
https://github.com/basanta-spring-boot/documents/tree/main
